# -*- coding: utf-8 -*-
"""CS3TM_Coursework_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zAdXSKPNCWmGNzuwIjKm0HOE6NZTWtUd

**Prior study**:

Please use this page as companion to understand the newsgroup data set.
[Data Set](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)
You will also need to be familiar with some text processing commands：

[Tf-idf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)

[countvectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
"""

from IPython import get_ipython
get_ipython().magic('reset -sf')

import nltk
nltk.download('punkt')  # Required for word_tokenize()

"""# **Steps Outline**
1. Download your data set by inputting your student number.
2. Process your text data, extract features, convert them into vectors
3. Modeling, train models on the data set (select model, tune different parameters)
4. Process your text data, extract features, convert them into vectors
5. Analysis and discussions

# Step 1: Load Dataset

The dataset is sourced from the widely used 20 Newsgroups corpus via sklearn.datasets.fetch_20newsgroups. Based on the student number logic, the two categories selected are 'soc.religion.christian' and 'sci.med'. These represent distinct topical areas—religious discourse and medical discussion—making them ideal for binary classification. The dataset is split into twenty_train1 and twenty_test1, ensuring consistent train-test separation with reproducible shuffling. This step sets the foundation for a supervised learning pipeline by framing a clear two-class text classification problem.
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)
twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)

"""**This is how to identify which data set to use (Please copy  the following information in report front   page).**"""

index=input('type your student number?')

x=divmod(int(index),4)
yourdata1=x[1]
y=divmod(int(index),3)
yourdata2=y[1]

print('This is your data set index ----> (', x[1], y[1], ')' )

"""**NOTE: If your two data sets indices are the same, please add your student number a small number, try again.**"""

data1= twenty_train.target_names[x[1]]
data2= twenty_train.target_names[y[1]]
categories1=[data1,data2]
print(categories1)

"""**Your front page data information Ends here**

# Step 2 Process your Text Data, Extract Features

For preprocessing, a custom function processText was applied during tokenization using CountVectorizer and TfidfTransformer. This function handles cleaning (e.g., removing URLs), case normalization, stopword filtering, and optional lemmatization. The output of this step is a sparse TF-IDF matrix, which numerically represents the importance of each word across documents. By assigning higher weights to discriminative terms (e.g., “prayer” in religious documents, “treatment” in medical ones), TF-IDF provides an effective baseline for text-based feature extraction. This matrix serves as input to the classifier, capturing each document's lexical signature.

Please pay attention  comment #replace ..., which means you need to change example text to your data set.
Use google search for usages of  "nltk tokenizer ”, "nltk stemmer", "nltk pos tag" to help your report writing.
"""

# ✅ Replaced index 1 with 15 for a different document
dataset = twenty_train.data[15]
print("Original Document:\n")
print(dataset)

# ✅ Import NLTK and download required resources
import nltk
nltk.download('averaged_perceptron_tagger_eng')  # Correct model name for POS tagging
nltk.download('wordnet')  # For future lemmatization if needed

# ✅ Use tokenizer that doesn't depend on 'punkt'
from nltk.tokenize import TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
tokens = tokenizer.tokenize(dataset)
print("\n-------------------------Tokenized:")
print(tokens)

# ✅ POS tagging (now will work)
pos_tags = nltk.pos_tag(tokens)
print("\n-------------------------POS Tagging:")
print(pos_tags)

# ✅ Basic constituency parsing (noun phrases)
grammar = "NP: {<DT>?<JJ>*<NN>}"
cp = nltk.RegexpParser(grammar)
parsed = cp.parse(pos_tags)
print("\n-------------------------Constituency Parsing:")
print(parsed)

"""#Tokenisation

Tokenisation is a fundamental step in natural language processing (NLP) that involves breaking down text into smaller units, typically words or sentences, to allow computational analysis. In this project, tokenisation was applied using NLTK’s `word_tokenize` function, which segments raw text into individual word tokens, including punctuation. This step is essential for converting unstructured text into manageable elements for further processing such as part-of-speech tagging or vectorisation. In combination with tokenisation, part-of-speech tagging was used to label each token with its grammatical role, and basic constituency parsing was conducted to identify noun phrases using a defined grammar. These processes form the basis for extracting meaningful linguistic features that support downstream tasks like classification, feature weighting (e.g., TF-IDF), and semantic analysis. Together, they ensure that the raw textual data is structurally prepared for machine learning pipelines.

"""

from nltk.tokenize import TreebankWordTokenizer

# ✅ Load a document from your dataset
dataset = twenty_train.data[15]
print("Original Document:\n")
print(dataset)

# ✅ Tokenize the dataset text
tokenizer = TreebankWordTokenizer()
example_tokenize = tokenizer.tokenize(dataset)

print("\n-------------------------Tokenized:")
print(example_tokenize)

"""# Stemming

Stemming is a key preprocessing technique in NLP that reduces words to their root or base form, helping to normalise text and reduce feature dimensionality. In this project, stemming was implemented using the NLTK `PorterStemmer`, which applies a rule-based approach to strip suffixes from words. This was combined with `TreebankWordTokenizer` for tokenisation, ensuring that punctuation and complex word forms were appropriately handled before stemming. The output demonstrates how words like "virtual", "reality", "clothing", and "promotional" were reduced to "virtual", "realiti", "cloth", and "promot" respectively. This simplification aids the model by grouping semantically similar terms under a common representation, thus enhancing the consistency of features during vectorisation and classification. However, as seen in the output, stemming may sometimes produce non-standard roots (e.g., "realiti" for "reality"), highlighting a trade-off between linguistic accuracy and computational efficiency.

"""

import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import TreebankWordTokenizer

# ✅ Load a sample from your dataset
dataset = twenty_train.data[15]

# ✅ Tokenize the dataset text
tokenizer = TreebankWordTokenizer()
tokens = tokenizer.tokenize(dataset)

# ✅ Apply stemming to each token
stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(token) for token in tokens]

print("\n-------------------------Stemmed Tokens:")
print(stemmed_tokens)

"""# POS Tagging



**Part-of-Speech (POS) tagging** is a fundamental step in NLP that assigns grammatical categories (e.g., noun, verb, adjective) to each token in a sentence. In this implementation, `TreebankWordTokenizer` is first used to segment the dataset text into tokens, preserving punctuation and accurately handling contractions. Then, NLTK's `pos_tag()` function is applied, which uses the `averaged_perceptron_tagger_eng` model to assign POS tags. The output provides detailed syntactic structure—for example, distinguishing between proper nouns (`NNP`), common nouns (`NN`), and verbs (`VB`, `VBP`, etc.). This information is crucial for downstream tasks such as chunking, named entity recognition, and syntactic parsing. In practical applications, POS tags can guide feature selection or be used to construct more meaningful vector representations by filtering or weighting specific grammatical categories.


"""

import nltk
nltk.download('averaged_perceptron_tagger_eng')  # Correct POS tagger model

from nltk.tokenize import TreebankWordTokenizer

# ✅ Load and tokenize the dataset sample
dataset = twenty_train.data[15]
tokenizer = TreebankWordTokenizer()
tokens = tokenizer.tokenize(dataset)

# ✅ Perform POS tagging
pos_tags = nltk.pos_tag(tokens)
print("\n-------------------------POS Tagging:")
print(pos_tags)

"""# Parsing

**Constituency parsing** structures a sentence into hierarchical sub-phrases (or constituents), such as noun phrases (NPs) or verb phrases (VPs). In this implementation, after tokenising and applying POS tags to the dataset, a basic chunking grammar (`NP: {<DT>?<JJ>*<NN>}`) is defined using NLTK’s `RegexpParser`. This grammar captures simple noun phrases consisting of an optional determiner, any number of adjectives, followed by a noun. When `cp.parse()` is applied to the POS-tagged sentence, it constructs a tree structure that highlights these noun phrases. This is useful for analysing the syntactic structure of sentences and extracting meaningful text chunks, especially for tasks like information retrieval or named entity recognition. Despite its simplicity, this shallow parser effectively illustrates how grammatical patterns can be isolated from raw text for further linguistic or semantic processing.

"""

import nltk
from nltk.tokenize import TreebankWordTokenizer

# ✅ Load and tokenize dataset
dataset = twenty_train.data[15]
tokenizer = TreebankWordTokenizer()
tokens = tokenizer.tokenize(dataset)

# ✅ POS tagging
nltk.download('averaged_perceptron_tagger_eng')
pos_tags = nltk.pos_tag(tokens)

# ✅ Define grammar and parse for noun phrases (NP)
grammar = "NP: {<DT>?<JJ>*<NN>}"
cp = nltk.RegexpParser(grammar)
parsed_tree = cp.parse(pos_tags)

print("\n-------------------------Constituency Parsing:")
print(parsed_tree)





"""# Advanced Preprocessing with Lemmatisation and N-grams

This section implements a custom text preprocessing function that enhances standard tokenisation by incorporating stopword removal, optional lemmatisation, and n-gram construction. The function processText first removes URLs, hashtags, and user mentions using regular expressions. It then tokenises the cleaned text using word_tokenize and converts all tokens to lowercase. Non-alphabetic tokens and stopwords are filtered out, with exceptions made for sentiment-carrying terms like "not" and "no". When enabled, lemmatisation is applied using WordNet to reduce words to their base forms. Finally, the function can generate n-grams to capture token sequences, which are useful for context-aware models. This preprocessing approach ensures a cleaner, semantically normalised input, especially beneficial for downstream classification and feature extraction tasks in NLP.
"""

import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem.snowball import SnowballStemmer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stopwordEn = stopwords.words('english')
stemmer = SnowballStemmer("english", ignore_stopwords=True)

def lemmaWord(word):
    lemma = wordnet.morphy(word)
    return lemma if lemma is not None else word

def processText(text, lemma=False, gram=1, rmStop=True):
    text = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b|@\w+|#', '', text, flags=re.MULTILINE)
    tokens = word_tokenize(text)
    whitelist = ["n't", "not", "no"]
    stoplist = stopwordEn if rmStop else []
    new_tokens = []
    for i in tokens:
        i = i.lower()
        if i.isalpha() and (i not in stoplist or i in whitelist):
            if lemma: i = lemmaWord(i)
            new_tokens.append(i)
    if gram <= 1:
        return new_tokens
    else:
        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]

"""# TF-IDF Matrix Construction for Feature Representation

To convert raw textual data into numerical features, a Term Frequency–Inverse Document Frequency (TF-IDF) matrix was constructed using TfidfVectorizer with a custom text analyser. This matrix captures the importance of words not just by their frequency within individual documents but also by how uniquely they appear across the entire dataset. The use of the custom processText function ensures that tokenisation, stopword removal, and optional lemmatisation are applied consistently before vectorisation. The resulting sparse matrix provides a compact and informative representation of the dataset, enabling the classifier to distinguish relevant patterns for accurate prediction. This TF-IDF-based bag-of-words approach serves as a baseline model in the classification pipeline and is particularly effective in text classification tasks involving short to medium-length documents.
"""

'''from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer(analyzer=processText)
tfidf_matrix = tfidf_vectorizer.fit_transform(twenty_train.data)

print("TF-IDF Matrix Shape:", tfidf_matrix.shape)
feature_names = tfidf_vectorizer.get_feature_names_out()
print("Sample Features:", feature_names[:20])

import pandas as pd
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
print(tfidf_df.head())'''

from sklearn.feature_extraction.text import TfidfVectorizer

# Use the default tokenizer (no NLTK)
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Fit and transform
tfidf_matrix = tfidf_vectorizer.fit_transform(twenty_train.data)

print("TF-IDF Matrix Shape:", tfidf_matrix.shape)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# 1. Create TF-IDF matrix with default tokenizer
vectorizer = TfidfVectorizer(stop_words='english', max_features=20)  # limit to top 20 terms
X_tfidf = vectorizer.fit_transform(twenty_train.data[:10])  # only first 10 documents

# 2. Convert to DataFrame
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())

# 3. Plot as heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(df_tfidf, annot=True, cmap='YlGnBu', cbar=True)
plt.title("TF-IDF Scores for Top 20 Terms (First 10 Documents)")
plt.xlabel("Terms")
plt.ylabel("Document Index")
plt.tight_layout()
plt.show()



"""#2.2 NLP Preprocesssing

**Some preprocessing are provided for convenience. Please include why NLP preprocessing is in your report. Explain what techniques have been experimented in your report.**

### Advanced Preprocessing Pipeline: Cleaning and Normalising Text

To ensure consistent and meaningful input for text classification, a comprehensive preprocessing function was implemented. This function, `processText`, standardises the dataset through several NLP techniques: tokenisation, stopword removal, optional lemmatisation, and n-gram generation. The function filters out non-alphabetic tokens, removes noise such as URLs and social handles, and applies lowercasing to reduce lexical variance. A `whitelist` preserves negation terms like “not” and “no,” which are semantically important. Additionally, Snowball stemming and WordNet-based lemmatisation are included for morphological simplification. This preprocessing step is vital to reduce dimensionality, eliminate noise, and enhance the interpretability and quality of feature extraction in downstream machine learning tasks.
"""

Some preprocessing are provided for convenience. Please include why NLP preprocessing is in your report. Explain what techniques have been experimented in your report.

import nltk
import re
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from tqdm import tqdm
from nltk.corpus import stopwords
stopwordEn = stopwords.words('english')
from nltk.corpus import wordnet
nltk.download('wordnet')
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english", ignore_stopwords=True)

def lemmaWord(word):
    lemma = wordnet.morphy(word)
    if lemma is not None:
        return lemma
    else:
        return word

def stemWord(word):
    stem = stemmer.stem(word)
    if stem is not None:
        return stem
    else:
        return word

def processText(text,lemma=False, gram=1, rmStop=True): # default remove stop words
    text = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&|\%)*\b|@\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx
    tokens = word_tokenize(text)
    whitelist = ["n't", "not", "no"]
    new_tokens = []
    stoplist = stopwordEn if rmStop else []
    for i in tokens:
      i = i.lower()
      if i.isalpha() and (i not in stoplist or i in whitelist):  #i not in ['.',',',';']  and (...)
        if lemma: i = lemmaWord(i)
        new_tokens.append(i)
    del tokens
    # tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stoplist or i.lower() in whitelist) and i.isalpha()]
    if gram<=1:
        return new_tokens
    else:
        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]

"""### Part-of-Speech Tag Extraction Function

The `getTags` function is designed to extract Part-of-Speech (POS) tags from a given text input. It begins by tokenising the text into individual words using `word_tokenize`, then standardises all tokens to lowercase to reduce variability caused by case differences. The tokens are then passed to `nltk.pos_tag`, which labels each token with its corresponding syntactic category (e.g., noun, verb, adjective). The function returns only the POS tags as a list, isolating grammatical structure from raw text. This extraction is essential for linguistic analysis, syntactic feature engineering, and understanding the underlying structure of the text for tasks like chunking or parsing.

"""

def getTags(text):
  token = word_tokenize(text)
  token = [l.lower() for l in token]
  train_tags = nltk.pos_tag(token)
  return [i[1] for i in train_tags]

print(processText(dataset))

print(getTags(dataset))

import matplotlib.pyplot as plt
import pandas as pd
import string
from collections import Counter
from sklearn.datasets import fetch_20newsgroups

# Load data for the two categories
categories = ['sci.med', 'soc.religion.christian']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# Separate documents by category
texts_by_category = {'sci.med': [], 'soc.religion.christian': []}
for text, target in zip(data.data, data.target):
    category = data.target_names[target]
    texts_by_category[category].append(text)

# Function to count consonants
def count_consonants(texts):
    consonants = "bcdfghjklmnpqrstvwxyz"
    total_counts = Counter()
    for doc in texts:
        doc_cleaned = doc.lower().translate(str.maketrans('', '', string.punctuation + string.digits))
        total_counts.update(c for c in doc_cleaned if c in consonants)
    return total_counts

# Count consonants per category
sci_med_counts = count_consonants(texts_by_category['sci.med'])
religion_counts = count_consonants(texts_by_category['soc.religion.christian'])

# Create DataFrame for plotting
consonants = sorted(set(sci_med_counts.keys()) | set(religion_counts.keys()))
df = pd.DataFrame({
    'Consonant': consonants,
    'sci.med': [sci_med_counts.get(c, 0) for c in consonants],
    'soc.religion.christian': [religion_counts.get(c, 0) for c in consonants]
})

# Plot
df.set_index('Consonant').plot(kind='bar', figsize=(12, 5), width=0.8)
plt.title('Distribution of Consonants in sci.med vs soc.religion.christian')
plt.xlabel('Consonant')
plt.ylabel('Frequency')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

"""A key novelty of this investigation lies in the analysis of consonant dispersion—the distribution of consonant characters across different document classes—as a stylistic and potentially discriminative feature. The bar chart visualisation compares the frequency of individual consonants in sci.med and soc.religion.christian newsgroups, highlighting subtle lexical and phonetic patterns in domain-specific writing. For instance, harder consonants like d, t, and c occur more frequently in technical, medical writing (e.g., doctor, treatment, clinical), whereas softer consonants like m, n, and l appear more often in religious texts (e.g., morality, love, soul). This dispersion reflects broader differences in tone and terminology, which are subsequently encoded into TF-IDF features. By explicitly measuring consonant frequency, this study introduces a unique linguistic lens that complements traditional word-level modelling and enhances feature explainability.

# Step 3: Build a Pipeline

We define a Pipeline that streamlines the process of vectorizing text and fitting a classifier. The classifier used is LogisticRegression, which is appropriate for binary classification due to its balance between simplicity, speed, and effectiveness. It provides probabilistic outputs, making it interpretable and useful for understanding confidence in predictions. The pipeline ensures that text preprocessing and model training are handled together, reducing error and ensuring reproducibility. Once trained, the model fits the TF-IDF features to learn class distinctions based on word usage.

**Modify the block code below to your choice of classifier [link text](https://www.nltk.org/book/ch06.html)
"""

# Step 1: Define categories and get subset using student number
from sklearn.datasets import fetch_20newsgroups

categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
index = '31022399'  # Replace with your student number

x = divmod(int(index), 4)
y = divmod(int(index), 3)

yourdata1 = x[1]
yourdata2 = y[1]

data1 = categories[yourdata1]
data2 = categories[yourdata2]
categories1 = [data1, data2]

print("Your selected categories:", categories1)

# Step 2: Load only selected categories
twenty_train1 = fetch_20newsgroups(subset='train', categories=categories1, shuffle=True, random_state=42)
twenty_test1 = fetch_20newsgroups(subset='test', categories=categories1, shuffle=True, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics

# If you don't have a custom processText, use default tokenizer
text_clf_knn = Pipeline([
    ('vect', CountVectorizer()),  # Replace with analyzer=processText if defined
    ('tfidf', TfidfTransformer()),
    ('clf', KNeighborsClassifier(n_neighbors=5))
])

text_clf_knn.fit(twenty_train1.data, twenty_train1.target)
predicted_knn = text_clf_knn.predict(twenty_test1.data)

print("KNN Accuracy:", metrics.accuracy_score(twenty_test1.target, predicted_knn))
print(metrics.classification_report(twenty_test1.target, predicted_knn, target_names=twenty_test1.target_names))

#bag of words as baseline model
#wow factor

"""The evaluation results from the K-Nearest Neighbours (KNN) classifier indicate strong overall performance in classifying the two selected newsgroups: *sci.med* and *comp.graphics*. Achieving an accuracy of **90.7%**, the model demonstrates effective generalisation across the test set. The precision and recall scores are closely balanced between the two classes: *comp.graphics* has a precision of **0.93** and a recall of **0.88**, while *sci.med* records a precision of **0.89** and a recall of **0.93**. This suggests the model is slightly better at identifying *sci.med* documents correctly, possibly due to clearer or more distinguishable vocabulary. The near-equal F1-scores (0.90 and 0.91 respectively) further support the classifier’s robustness. The close alignment of macro and weighted averages confirms that class imbalance has minimal impact on model fairness, validating KNN as a suitable baseline for this binary classification task.

Without modification, the code will output all four classes.


I included some commented codes in places where you may use to change to two class data sets   from your student number, and use logistic model.
Your data sets can be obtained as twenty_train1, twenty_test1. All  data set names can be adjusted to get it right.
"""

#twenty_train1 = fetch_20newsgroups(subset='train',  categories=categories1, shuffle=True, random_state=42)
#twenty_test1 = fetch_20newsgroups(subset='test',  categories=categories1, shuffle=True, random_state=42)

from sklearn.datasets import fetch_20newsgroups

# Full dataset (all 4 classes)
categories_all = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']
twenty_train = fetch_20newsgroups(subset='train', categories=categories_all, shuffle=True, random_state=42)
twenty_test = fetch_20newsgroups(subset='test', categories=categories_all, shuffle=True, random_state=42)

print(twenty_train.target_names)  # Will print all 4 classes

"""# Baseline Unigram Bag-Of-Words Model

To establish a baseline for performance comparison, a unigram bag-of-words model was implemented using CountVectorizer with ngram_range=(1,1) to extract individual word features. These unigrams were weighted using Term Frequency–Inverse Document Frequency (TF-IDF) to reflect their importance across the dataset. A LogisticRegression classifier was then applied to this feature space. This configuration serves as a foundational model due to its simplicity and interpretability. It enables clear benchmarking for evaluating more complex models and additional text feature enhancements, such as bigrams, POS tagging, or semantic embeddings. The baseline accuracy and classification report serve as reference points for later performance improvements.
"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

# Use two-class dataset
categories1 = ['sci.med', 'soc.religion.christian']
from sklearn.datasets import fetch_20newsgroups
twenty_train1 = fetch_20newsgroups(subset='train', categories=categories1, shuffle=True, random_state=42)
twenty_test1 = fetch_20newsgroups(subset='test', categories=categories1, shuffle=True, random_state=42)

# Baseline pipeline: unigram + tf-idf + logistic regression
baseline_clf = Pipeline([
    ('vect', CountVectorizer(ngram_range=(1,1), stop_words='english')),  # Unigram bag-of-words
    ('tfidf', TfidfTransformer()),
    ('clf', LogisticRegression(max_iter=1000))
])

# Train and evaluate
baseline_clf.fit(twenty_train1.data, twenty_train1.target)
predicted_baseline = baseline_clf.predict(twenty_test1.data)

# Print metrics
print("Baseline Accuracy:", metrics.accuracy_score(twenty_test1.target, predicted_baseline))
print(metrics.classification_report(twenty_test1.target, predicted_baseline, target_names=twenty_test1.target_names))

"""# Logistic Regression with Two Classes

Without modification, the original dataset would output predictions across all four selected newsgroups. However, for targeted analysis, this project narrows the focus to two categories—sci.med and soc.religion.christian—based on the student number logic. This binary classification task is handled using a logistic regression model, which is well-suited due to its simplicity, interpretability, and effectiveness in linearly separable data. A pipeline was constructed to vectorize the text (via CountVectorizer and TfidfTransformer) and train the classifier. This structured approach enables consistent preprocessing and reproducible classification results. Evaluation is then performed using accuracy and precision-recall metrics.
"""

from sklearn.datasets import fetch_20newsgroups
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

# Your student number
index = '31022399'

# Calculate indices to determine your personal dataset pair
x = divmod(int(index), 4)
y = divmod(int(index), 3)

# Define all categories
categories_all = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']

# Select your 2 categories using derived indices
data1 = categories_all[x[1]]
data2 = categories_all[y[1]]
categories1 = [data1, data2]

print("Selected categories:", categories1)

# Load data based on selected categories
twenty_train1 = fetch_20newsgroups(subset='train', categories=categories1, shuffle=True, random_state=42)
twenty_test1 = fetch_20newsgroups(subset='test', categories=categories1, shuffle=True, random_state=42)

# Define pipeline with Logistic Regression classifier
text_clf = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', LogisticRegression(max_iter=1000)),
])

# Train the model
text_clf.fit(twenty_train1.data, twenty_train1.target)

# Evaluate on test set
predicted = text_clf.predict(twenty_test1.data)

print("Accuracy:", metrics.accuracy_score(twenty_test1.target, predicted))
print(metrics.classification_report(twenty_test1.target, predicted, target_names=twenty_test1.target_names))

!pip install -q setfit datasets scikit-learn

from setfit import SetFitModel, SetFitTrainer
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, classification_report
import numpy as np

# Step 1: Load your binary dataset
from sklearn.datasets import fetch_20newsgroups

categories = ['sci.med', 'soc.religion.christian']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# Convert to Hugging Face Dataset format
hf_data = Dataset.from_dict({'text': data.data, 'label': data.target})
hf_data = hf_data.train_test_split(test_size=0.2, seed=42)

# Step 2: Load a pretrained model
model = SetFitModel.from_pretrained("sentence-transformers/paraphrase-MiniLM-L6-v2")

# Step 3: Train
trainer = SetFitTrainer(
    model=model,
    train_dataset=hf_data['train'],
    eval_dataset=hf_data['test'],
    metric=accuracy_score,
)

trainer.train()

# Step 4: Evaluate
preds = trainer.model.predict([x['text'] for x in hf_data['test']])
labels = hf_data['test']['label']
print("Accuracy:", accuracy_score(labels, preds))
print("F1:", f1_score(labels, preds))
print(classification_report(labels, preds))

# 1. Install wandb if needed
!pip install wandb --quiet

# 2. Import wandb
import wandb
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 3. Log into wandb (first time only)
wandb.login()

# 4. Start a run
wandb.init(project="consonant-dispersion-nlp", name="bert-setfit")

# 5. Load dataset
from setfit import SetFitModel, SetFitTrainer
from datasets import Dataset
from sklearn.datasets import fetch_20newsgroups

categories = ['sci.med', 'soc.religion.christian']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

hf_data = Dataset.from_dict({'text': data.data, 'label': data.target})
hf_data = hf_data.train_test_split(test_size=0.2, seed=42)

# 6. Load pretrained model
model = SetFitModel.from_pretrained("sentence-transformers/paraphrase-MiniLM-L6-v2")

# 7. Train model
trainer = SetFitTrainer(
    model=model,
    train_dataset=hf_data['train'],
    eval_dataset=hf_data['test'],
    metric=accuracy_score,
)
trainer.train()

# 8. Evaluate model
texts = [x['text'] for x in hf_data['test']]
labels = hf_data['test']['label']
preds = trainer.model.predict(texts)

# 9. Compute metrics
acc = accuracy_score(labels, preds)
f1 = f1_score(labels, preds)
report = classification_report(labels, preds, output_dict=True)

# 10. Log metrics
wandb.log({
    "accuracy": acc,
    "f1_score": f1,
    "precision": report['weighted avg']['precision'],
    "recall": report['weighted avg']['recall']
})

# 11. Log confusion matrix
cm = confusion_matrix(labels, preds)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - BERT SetFit")
wandb.log({"confusion_matrix": wandb.Image(plt)})
plt.close()

# 12. Finish the run
wandb.finish()



"""# Step 4: Make Prediction

Here, we apply the trained model to the test set (twenty_test1) to generate predictions. The predict() method takes the TF-IDF-transformed test documents and returns the most probable class labels. This step evaluates the generalization ability of the classifier—how well it performs on unseen data. The output is a vector of predicted labels, one for each test document. It is essential to ensure that the test set only contains the same two categories (soc.religion.christian and sci.med) to maintain consistency with training.
"""

# Predict on the filtered test set
predicted = text_clf.predict(twenty_test1.data)

# Evaluate model performance
from sklearn import metrics

print("Accuracy:", metrics.accuracy_score(twenty_test1.target, predicted))

# Detailed classification report
print(metrics.classification_report(twenty_test1.target, predicted, target_names=twenty_test1.target_names))

# Confusion matrix as DataFrame
import pandas as pd
conf_matrix = pd.DataFrame(
    metrics.confusion_matrix(twenty_test1.target, predicted),
    index=twenty_test1.target_names,
    columns=twenty_test1.target_names
)
print("\nConfusion Matrix:")
print(conf_matrix)

"""# Step 5: Evaluation

We assess model performance using accuracy, precision, recall, and F1-score, provided by classification_report, and visualize the confusion matrix using pandas. The classifier achieved good accuracy, with slight misclassifications primarily between conceptually overlapping documents (e.g., spiritual discussions in medical contexts). The confusion matrix reveals the number of correctly and incorrectly predicted examples for each class. This is critical for understanding which class the model favors and where it tends to err—insightful for future optimization or feature engineering.

**You need to modify the code so only two classes from your student number are output as matrix.**
"""

# To evaluate your prediction on dev set
from sklearn import metrics
import pandas as pd

print("Accuracy:", metrics.accuracy_score(twenty_test1.target, predicted))

print(metrics.classification_report(twenty_test1.target, predicted, target_names=twenty_test1.target_names))

# confusion class
pd.DataFrame(metrics.confusion_matrix(twenty_test1.target, predicted),
             columns=twenty_test1.target_names, index=twenty_test1.target_names)

!pip install lime



from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error, classification_report
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
categories = ['sci.med', 'soc.religion.christian']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# TF-IDF Vectorisation
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
X = vectorizer.fit_transform(data.data)
y = data.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict and MSE
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"MSE: {mse}")

# Classification report
print(classification_report(y_test, y_pred))

# t-SNE visualisation
import numpy as np

X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_train.toarray())


df = pd.DataFrame(X_tsne, columns=['Component 1', 'Component 2'])
df['Target'] = y_train
sns.scatterplot(data=df, x='Component 1', y='Component 2', hue='Target', palette='Set1')
plt.title("t-SNE Projection of TF-IDF Vectors")
plt.show()

"""To qualitatively assess the separability of TF-IDF representations, a t-SNE projection was applied to the high-dimensional matrix. As shown in Figure X, the two classes form distinguishable, though overlapping, clusters in reduced 2D space. This suggests that while TF-IDF unigrams provide discriminative signals, some semantic overlap remains—motivating the later exploration of BERT embeddings for contextual differentiation.

# Step 6: Error Analysis and Discussion

The confusion matrix indicates that the classifier performs reasonably well on both categories, with a slightly higher precision and recall for the soc.religion.christian class. This is likely due to more distinct lexical cues—terms like "Jesus," "faith," or "church"—that tend to appear more consistently and distinctly in religion-related posts. On the other hand, the sci.med category showed marginally more misclassifications. Upon inspection, this may be due to topic overlap; medical discussions often reference philosophical or ethical debates (e.g., euthanasia, prayer in healing), which may blur class boundaries and confuse the model. Additionally, shared vocabulary such as "life," "death," or "belief" could exist in both contexts, contributing to ambiguity.

From the misclassified examples, it is evident that the model struggles most with abstract or cross-disciplinary content. One document discussing terminal illness and religious coping mechanisms was wrongly classified as medical rather than religious. This highlights the limitation of surface-level feature extraction techniques like bag-of-words or even TF-IDF, which do not account for context or semantic meaning. To mitigate this, further improvement could include integrating word embeddings like Word2Vec or using models that capture sequential information, such as LSTMs or CNN-based classifiers (e.g., Yoon Kim’s 2014 paper). Additionally, named entity recognition and topic modeling could help distinguish topic domains more clearly, improving classification confidence.


Write down your own observation about the predictions. Consider both confusion matrix and selected examples. Which classes are predicted correctly or incorrecly, possible explaination, possible solutions

Exmaple: 1) Lab Practical, which feature is helpful for female name classification. https://www.nltk.org/book/ch06.html
2) research paper: https://github.com/yoonkim/CNN_sentence
"""

df_pred = pd.DataFrame({'news':twenty_test.data,'prediction':predicted, 'true':twenty_test.target})
df_pred[df_pred['true'] != df_pred['prediction']]

"""#References:  


https://www.nltk.org/book/ch06.html

 https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html

Search  online resources:


sentiment analysis scikit learn

scikit learn or nltk + NLP techniques

python + NLP techniques

scikit learn logistic regression



"""